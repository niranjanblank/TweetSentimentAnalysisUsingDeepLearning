{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:22:02.630556Z",
     "start_time": "2023-05-29T10:21:50.334749300Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helper_functions import pre_process_tweet\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec.load(\"trained_models/word2vec.model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:22:05.829202500Z",
     "start_time": "2023-05-29T10:22:02.630556Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# get the data\n",
    "data = pd.read_csv('data.csv',encoding='ISO-8859-1', header=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:22:08.292186100Z",
     "start_time": "2023-05-29T10:22:05.829202500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data.columns = ['target','ids','date','flag','user','text']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:22:08.294495100Z",
     "start_time": "2023-05-29T10:22:08.292186100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# get only the text and its label\n",
    "data_to_used =data[['target','text']]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:22:08.327662700Z",
     "start_time": "2023-05-29T10:22:08.295496300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# pre-process the texts\n",
    "preprocessed_tweet_data = []\n",
    "# iterating and tokenizing all the tweets\n",
    "for index, text in enumerate(data_to_used['text'], start=1):\n",
    "    tweet = pre_process_tweet(text)\n",
    "    preprocessed_tweet_data.append(tweet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:25:32.426562200Z",
     "start_time": "2023-05-29T10:22:08.325663100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "maxlen = max(len(x) for x in preprocessed_tweet_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:25:32.502031700Z",
     "start_time": "2023-05-29T10:25:32.427562Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# convert each word in sentence to embedding using word2vec model\n",
    "def word_to_vec(sentence):\n",
    "    return [word2vec.wv[word] if word in word2vec.wv else np.zeros(word2vec.vector_size) for word in sentence]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:25:32.505072100Z",
     "start_time": "2023-05-29T10:25:32.503031700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# convert the tokenized words to embeddings\n",
    "vectorized_tweets = [word_to_vec(sentence) for sentence in preprocessed_tweet_data]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:25:43.977099600Z",
     "start_time": "2023-05-29T10:25:32.505072100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "# separate the data into train and test set\n",
    "# First, compute the maximum length of any tweet in your dataset\n",
    "maxlen = max(len(tweet) for tweet in vectorized_tweets)\n",
    "print(maxlen)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:25:44.022978700Z",
     "start_time": "2023-05-29T10:25:43.977099600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "# used to have uniform input by padding the input which are smaller\n",
    "def pad_sequence(seq, maxlen):\n",
    "    return np.array(seq + [np.zeros(word2vec.vector_size)] * (maxlen - len(seq)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:25:44.026049300Z",
     "start_time": "2023-05-29T10:25:44.021978500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Then, use a list comprehension to pad every sequence in vectorized_tweets\n",
    "vectorized_tweets_padded = [pad_sequence(tweet, maxlen) for tweet in vectorized_tweets]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:25.732945400Z",
     "start_time": "2023-05-29T10:25:44.026049300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:25.742809600Z",
     "start_time": "2023-05-29T10:26:25.739949500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "labels = data['target'].values  # this will convert the pandas Series to a numpy array\n",
    "labels = [0 if label == 0 else 1 for label in labels]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:25.916394600Z",
     "start_time": "2023-05-29T10:26:25.753706900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "vectorized_tweets_train, vectorized_tweets_val, labels_train, labels_val = train_test_split(vectorized_tweets_padded, labels, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:26.572681500Z",
     "start_time": "2023-05-29T10:26:25.918394300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "list"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:26.651568500Z",
     "start_time": "2023-05-29T10:26:26.574682Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:26.656096600Z",
     "start_time": "2023-05-29T10:26:26.654591100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# this function is used to create generator to train and test\n",
    "def data_generator(vectorized_tweets, labels, batch_size=32):\n",
    "    data_size = len(vectorized_tweets)\n",
    "    indices = np.arange(data_size)\n",
    "    np.random.shuffle(indices)\n",
    "    while True:\n",
    "        for i in range(0, data_size, batch_size):\n",
    "            # Get batch indices\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "\n",
    "            # Get the batch of sequences and corresponding labels\n",
    "            x = [vectorized_tweets[i] for i in batch_indices]\n",
    "            y = [labels[i] for i in batch_indices]\n",
    "\n",
    "            # Convert lists to numpy arrays for Keras\n",
    "            x_array = np.asarray(x)\n",
    "            y_array = np.asarray(y)\n",
    "\n",
    "            yield x_array, y_array\n",
    "\n",
    "        # Re-shuffle indices for the next epoch\n",
    "        np.random.shuffle(indices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:26.692386300Z",
     "start_time": "2023-05-29T10:26:26.681097100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "train_generator = data_generator(vectorized_tweets_train, labels_train)\n",
    "val_generator = data_generator(vectorized_tweets_val, labels_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:26.725127200Z",
     "start_time": "2023-05-29T10:26:26.700386Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:26.735912600Z",
     "start_time": "2023-05-29T10:26:26.729127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:26.745580300Z",
     "start_time": "2023-05-29T10:26:26.742072500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(maxlen, embedding_dim)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:30.078557600Z",
     "start_time": "2023-05-29T10:26:26.747580800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40000/40000 [==============================] - 1787s 45ms/step - loss: 0.4677 - accuracy: 0.7785 - val_loss: 0.4490 - val_accuracy: 0.7888\n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 1775s 44ms/step - loss: 0.4442 - accuracy: 0.7919 - val_loss: 0.4419 - val_accuracy: 0.7912\n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 1770s 44ms/step - loss: 0.4368 - accuracy: 0.7964 - val_loss: 0.4390 - val_accuracy: 0.7932\n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 1797s 45ms/step - loss: 0.4319 - accuracy: 0.7991 - val_loss: 0.4387 - val_accuracy: 0.7948\n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 1754s 44ms/step - loss: 0.4287 - accuracy: 0.8007 - val_loss: 0.4370 - val_accuracy: 0.7951\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(vectorized_tweets_train) // 32\n",
    "validation_steps = len(vectorized_tweets_val) // 32\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=5\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T12:54:34.695693400Z",
     "start_time": "2023-05-29T10:26:30.078557600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def get_pre_processed_input(tweet,maxlen=37,embedding_dim=100):\n",
    "    # maxlen is 37\n",
    "    # embedding_dim is 100 which is the vector size of word2vec model\n",
    "    # Tokenize and pad the tweet\n",
    "    pre_processed_new_tweet = pre_process_tweet(tweet)\n",
    "    #getting embedding of each word of tweet\n",
    "    vectorized_new_tweet =  word_to_vec(pre_processed_new_tweet)\n",
    "    # padding to get uniform size inout\n",
    "    vectorized_new_tweet_padded = pad_sequence(vectorized_new_tweet, maxlen=maxlen)\n",
    "    # reshaping the tweet to match input shape of model\n",
    "    vectorized_new_tweet_padded = vectorized_new_tweet_padded.reshape(1, maxlen, embedding_dim)\n",
    "\n",
    "    return vectorized_new_tweet_padded\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T13:23:24.335927Z",
     "start_time": "2023-05-29T13:23:24.331970800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "new_tweet = \"I hate this movie!\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T13:25:44.495182200Z",
     "start_time": "2023-05-29T13:25:44.492259500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "processed_input = get_pre_processed_input(new_tweet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T13:25:44.859561800Z",
     "start_time": "2023-05-29T13:25:44.856191700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(processed_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T13:25:45.233308100Z",
     "start_time": "2023-05-29T13:25:45.197180200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.06353077]], dtype=float32)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T13:25:46.011174400Z",
     "start_time": "2023-05-29T13:25:46.008504400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model.save('trained_models/sentiment_model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-29T13:29:49.810979200Z",
     "start_time": "2023-05-29T13:29:49.662478200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
